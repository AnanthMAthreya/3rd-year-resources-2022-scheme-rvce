{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.data.path.append(r'C:\\Users\\anant\\AppData\\Roaming\\nltk_data') \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\anant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anant\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find('tokenizers/punkt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence,sentences):\n",
    "    #preprocess the text \n",
    "\n",
    "    def preprocess(text):\n",
    "        text=text.lower()\n",
    "        cleaned=[]\n",
    "        words=word_tokenize(text)\n",
    "        s_w=set(stopwords.words('english'))\n",
    "\n",
    "        for word in words:\n",
    "            if word not in s_w:\n",
    "                if word.isalnum():\n",
    "                    cleaned.append(word)\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "    #calculate tf in the whole document\n",
    "\n",
    "    def tf(word,sentence):\n",
    "        count=0\n",
    "        words=word_tokenize(sentence)\n",
    "        for w in words:\n",
    "            if word==w:\n",
    "                count=count+1\n",
    "        return count/len(words)\n",
    "        # words = sentence.split()\n",
    "        # return words.count(word) / (len(words))\n",
    "\n",
    "\n",
    "    #calculate idf in the whole docuement\n",
    "    def idf(word,sentences):\n",
    "        count=0\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                count+=1\n",
    "        return np.log(len(sentences)/(count+1))\n",
    "\n",
    "\n",
    "    words=preprocess(sentence)\n",
    "    tf_idf_score=0\n",
    "    for word in words:\n",
    "        tf_score=tf(word,sentence)\n",
    "        idf_scroe=idf(word,sentences)\n",
    "        tf_idf_score+=tf_score*idf_scroe\n",
    "\n",
    "    return tf_idf_score\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(text,length):\n",
    "    #this creates a dictionary to store sentence and score wrt to sentences and then later extract top_k sentences\n",
    "\n",
    "    sent_score={}\n",
    "\n",
    "    sentences=sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        score=tf_idf(sentence,sentences)\n",
    "        sent_score[sentence]=score\n",
    "    \n",
    "    sorted_sent_score = dict(sorted(sent_score.items(), key=lambda x: x[1], reverse=True))\n",
    "    top_k=list(sorted_sent_score.keys())[:length]\n",
    "    \n",
    "    summary=' '.join(top_k)\n",
    "    return summary\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dhoni retired from test cricket in 2014, but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs. In 2007, he became the captain of the ODI side before taking over in all formats by 2008.\n"
     ]
    }
   ],
   "source": [
    "#tf-idf sentence summary\n",
    "\n",
    "text=\"\"\"\n",
    "Born in Ranchi, Dhoni made his first class debut for Bihar in 1999. He made his debut for the Indian cricket team on 23 December 2004 in an ODI against Bangladesh and played his first test a year later against Sri Lanka. In 2007, he became the captain of the ODI side before taking over in all formats by 2008. Dhoni retired from test cricket in 2014, but continued playing in limited overs cricket till 2019. He has scored 17,266 runs in international cricket including 10,000 plus runs at an average of more than 50 in ODIs.\n",
    "\"\"\"\n",
    "summary=summarizer(text,3)\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
